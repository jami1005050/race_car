{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gymnasium[classic_control]\n",
        "!pip install gymnasium[mujoco]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKwnIdGYNsAM",
        "outputId": "f782f341-ef31-4d6d-9681-e8bbadc55a74"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376418 sha256=2eeb889681ad75d5618ffd1ed89c1b098a02a5a8f290fc3b7ca5f6c3b976460e\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[classic_control]) (2.6.1)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.36.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.11.0)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install numpngw\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmH3ClqgVpZE",
        "outputId": "2b619327-2b76-42a1-e33a-84abd1ea87a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpngw\n",
            "  Downloading numpngw-0.1.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from numpngw) (1.26.4)\n",
            "Downloading numpngw-0.1.4-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: numpngw\n",
            "Successfully installed numpngw-0.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Models and computation\n",
        "import torch # will use pyTorch to handle NN\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from random import sample\n",
        "\n",
        "# Visualization\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import Image\n",
        "from numpngw import write_apng\n",
        "\n",
        "# IO\n",
        "import gymnasium as gym\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torch.distributions import MultivariateNormal, Normal,Beta\n"
      ],
      "metadata": {
        "id": "n4J-1abAI1H9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI9nQOC2MVZ0"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True)\n",
        "# env"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.shape,env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUSt6OxUnocw",
        "outputId": "bd27e326-866e-4708-d9a3-80dd6e2710fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((96, 96, 3), Box([-1.  0.  0.], 1.0, (3,), float32))"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "s,_ = env.reset()\n",
        "# print(s)\n",
        "img = env.render()\n",
        "images.append(img)\n",
        "\n",
        "done = False\n",
        "steps = 0\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    steps += 1\n",
        "    # print(env.step(action))\n",
        "    obs, reward, done, _,_ = env.step(action)\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "    if steps > 160:\n",
        "      break\n",
        "env.close()"
      ],
      "metadata": {
        "id": "YjCh2TeUVMx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s,_ = env.reset()\n",
        "# print(s)\n",
        "img = env.render()\n",
        "images.append(img)\n",
        "plt.imshow(img)  # if state is a 3D tensor (e.g., (1, H, W, C))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tb4MJdD_YXu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_apng('anim.png', images, delay=20)\n",
        "Image(filename='anim.png')"
      ],
      "metadata": {
        "id": "DM6Uuc2gVW3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "def save_checkpoint(model, filename, mode=0):\n",
        "    \"\"\"\n",
        "    Save a model to a file in your colab space\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: your policy network\n",
        "    filename: the name of the checkpoint file\n",
        "    mode (int): determines where to store the file\n",
        "                --> 0: collab space 1: Google Drive\n",
        "    \"\"\"\n",
        "    if mode == 1:\n",
        "      drive.mount('/content/gdrive')\n",
        "      path = F\"/content/gdrive/My Drive/{filename}\"\n",
        "      torch.save(model.state_dict(), path)\n",
        "    else:\n",
        "      torch.save(model.state_dict(), filename)\n",
        "\n",
        "def export_to_local_drive(filename):\n",
        "    \"\"\"\n",
        "    Download a file to your local machine\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename: the name of the file\n",
        "    \"\"\"\n",
        "    files.download(filename)\n"
      ],
      "metadata": {
        "id": "0pq6A0YyI5YX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SGwu27DoI7XG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "# class FrameStackWrapper:\n",
        "#     def __init__(self, env, num_frames):\n",
        "#         self.env = env\n",
        "#         self.num_frames = num_frames\n",
        "#         self.frames = deque(maxlen=num_frames)\n",
        "\n",
        "#     def reset(self):\n",
        "#         state, info = self.env.reset()\n",
        "#         for _ in range(self.num_frames):\n",
        "#             normalized_state = self.normalize_state(state)\n",
        "#             self.frames.append(normalized_state)\n",
        "#             # self.frames.append(state)\n",
        "#         return np.concatenate(self.frames, axis=2), info\n",
        "\n",
        "#     def normalize_state(self,state):\n",
        "#       return (state - state.min()) / (state.max() - state.min() + 1e-8)\n",
        "\n",
        "#     def step(self, action):\n",
        "#         state, reward, done, truncated, info = self.env.step(action)\n",
        "#         normalized_state = self.normalize_state(state)\n",
        "#         self.frames.append(normalized_state)\n",
        "#         # self.frames.append(state)\n",
        "#         return np.concatenate(self.frames, axis=2), reward, done, truncated, info\n",
        "\n",
        "#     def render(self):\n",
        "#         return self.env.render()\n",
        "\n",
        "#     def close(self):\n",
        "#         self.env.close()\n",
        "\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "\n",
        "class FrameStackWrapper:\n",
        "    def __init__(self, env, num_frames):\n",
        "        \"\"\"\n",
        "        Environment wrapper for frame stacking and reward adjustments.\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.num_frames = num_frames\n",
        "        self.frames = deque(maxlen=num_frames)\n",
        "        self.reward_threshold = self.env.spec.reward_threshold\n",
        "\n",
        "    def reset(self):\n",
        "        self.counter = 0\n",
        "        self.av_r = self.reward_memory()\n",
        "\n",
        "        self.die = False\n",
        "        img_rgb, _ = self.env.reset()\n",
        "        img_grey = self.rgb2gray(img_rgb)\n",
        "        nor_img = self.normalize_state(img_grey)\n",
        "\n",
        "        self.frames = deque([nor_img] * self.num_frames, maxlen=self.num_frames)\n",
        "        return np.stack(self.frames), _\n",
        "\n",
        "    # def step(self, action, action_repeat=1):\n",
        "    #     total_reward = 0\n",
        "    #     for i in range(action_repeat):\n",
        "    #         img_rgb, reward, done, truncated, info = self.env.step(action)\n",
        "\n",
        "    #         if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
        "    #             reward -= 0.05\n",
        "\n",
        "    #         total_reward += reward\n",
        "\n",
        "    #         if self.av_r(reward) <= -0.1 or done:\n",
        "    #             done = True\n",
        "    #             break\n",
        "\n",
        "    #     # Calculate average reward\n",
        "    #     avg_reward = total_reward / (i + 1)  # i + 1 because i starts from 0\n",
        "\n",
        "    #     img_gray = self.rgb2gray(img_rgb)\n",
        "    #     img_norm = self.normalize_state(img_gray)\n",
        "\n",
        "    #     self.frames.append(img_norm)\n",
        "\n",
        "    #     return np.stack(self.frames), avg_reward, done, truncated, info\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0\n",
        "        for i in range(4):\n",
        "            img_rgb, reward, die, truncated ,_ = self.env.step(action)\n",
        "            # don't penalize \"die state\"\n",
        "            if die:\n",
        "                reward += 50\n",
        "            # green penalty\n",
        "            if np.mean(img_rgb[:, :, 1]) > 140.0:\n",
        "                reward -= 0.05\n",
        "            total_reward += reward\n",
        "            # if no reward recently, end the episode\n",
        "            done = True if self.av_r(reward) <= -0.1 else False\n",
        "            if done or die or truncated:\n",
        "                break\n",
        "        img_gray = self.rgb2gray(img_rgb)\n",
        "        img_norm = self.normalize_state(img_gray)\n",
        "        # avg_reward = total_reward / (i + 1)  # i + 1 because i starts from 0\n",
        "\n",
        "        self.frames.append(img_norm)\n",
        "        return np.stack(self.frames), total_reward, done, truncated,_\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def rgb2gray(rgb):\n",
        "        # rgb image -> gray [0, 1]\n",
        "        gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
        "        return gray\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        Renders the environment.\n",
        "        \"\"\"\n",
        "        return self.env.render()\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"\n",
        "        Closes the environment.\n",
        "        \"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "    def normalize_state(self,state):\n",
        "        return state/255\n",
        "\n",
        "    @staticmethod\n",
        "    def reward_memory():\n",
        "        # record reward for last 100 steps\n",
        "        count = 0\n",
        "        length = 50\n",
        "        history = np.zeros(length)\n",
        "\n",
        "        def memory(reward):\n",
        "            nonlocal count\n",
        "            history[count] = reward\n",
        "            count = (count + 1) % length\n",
        "            return np.mean(history)\n",
        "\n",
        "        return memory\n"
      ],
      "metadata": {
        "id": "AHv6fBuQmAzH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3_PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super(TD3_PolicyNetwork, self).__init__()\n",
        "        # Convolutional layers for feature extraction\n",
        "# Convolutional layers for feature extraction\n",
        "        self.cnn_base = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=4, stride=2),   # (96, 96, 3) -> (8, 47, 47)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2),               # (8, 47, 47) -> (16, 23, 23)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2),              # (16, 23, 23) -> (32, 11, 11)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),              # (32, 11, 11) -> (64, 5, 5)\n",
        "            nn.ReLU(),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
        "            nn.ReLU(),  # activation\n",
        "        )\n",
        "        # Calculate the flattened feature size\n",
        "        with torch.no_grad():\n",
        "            in_shape = [4,96,96]\n",
        "            dummy_input = torch.zeros(1, *in_shape)\n",
        "            # print(dummy_input.shape)\n",
        "\n",
        "            flattened_size = self.cnn_base(dummy_input).view(1, -1).shape[1]\n",
        "\n",
        "        # Fully connected layers for control\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(flattened_size, 128),  # First hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_dim) ,\n",
        "            nn.Softplus()# Output layer\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_base(x)\n",
        "        # x = x.view(-1, 256)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        action = self.fc1(x)\n",
        "        return action\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Add batch dimension\n",
        "        # print(state.shape)\n",
        "        if(len(state.shape)==3):\n",
        "          state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Add batch dimension\n",
        "        else:\n",
        "          state = torch.from_numpy(state).float().to(device)  # Add batch dimension\n",
        "        action = self.forward(state)\n",
        "        # print(action.shape)\n",
        "        return action.detach().numpy()[0]\n",
        "\n",
        "    def evaluate_action(self, state):\n",
        "        # state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Add batch dimension\n",
        "        # print(state.shape)\n",
        "        if(len(state.shape)==3):\n",
        "          state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Add batch dimension\n",
        "        else:\n",
        "          state = torch.from_numpy(state).float().to(device)  # Add batch dimension\n",
        "        action = self.forward(state)\n",
        "        # print(action.shape)\n",
        "        return action.detach().numpy()\n",
        "\n",
        "\n",
        "class TD3_ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_shape,action_dim):\n",
        "        super(TD3_ValueNetwork, self).__init__()\n",
        "        # Convolutional layers for feature extraction\n",
        "        self.cnn_base = nn.Sequential(\n",
        "            nn.Conv2d(4, 8, kernel_size=4, stride=2),   # (96, 96, 3) -> (8, 47, 47)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2),               # (8, 47, 47) -> (16, 23, 23)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2),              # (16, 23, 23) -> (32, 11, 11)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2),              # (32, 11, 11) -> (64, 5, 5)\n",
        "            nn.ReLU(),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
        "            nn.ReLU(),  # activation\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
        "            nn.ReLU(),  # activation\n",
        "        )\n",
        "        # Calculate the flattened feature size\n",
        "        with torch.no_grad():\n",
        "            in_shape = [4,96,96]\n",
        "\n",
        "            dummy_input = torch.zeros(1, *in_shape)\n",
        "            # print(dummy_input.shape)\n",
        "            flattened_size = self.cnn_base(dummy_input).view(1, -1).shape[1]\n",
        "\n",
        "        # Fully connected layers for control\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(flattened_size+ action_dim, 128),  # First hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)       # Output layer\n",
        "        )# output shape (256, 1, 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, state,action):\n",
        "        if(len(state.shape)==3):\n",
        "          state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Add batch dimension\n",
        "        else:\n",
        "          state = torch.from_numpy(state).float().to(device)  # Add batch dimension\n",
        "        x = self.cnn_base(state)\n",
        "        # x = x.view(-1, 256)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # print(x.shape,action.shape)\n",
        "        x = torch.cat([x, action], dim=1)  # Concatenate along feature dimension\n",
        "\n",
        "        value = self.fc1(x)\n",
        "        return value\n"
      ],
      "metadata": {
        "id": "2q_1RpcpTnp-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, state_dim, action_dim):\n",
        "        self.max_size = max_size\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        return (np.array(states),\n",
        "                np.array(actions),\n",
        "                np.array(rewards).reshape(-1, 1),\n",
        "                np.array(next_states),\n",
        "                np.array(dones).reshape(-1, 1))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "######################## Your code ####################################\n",
        "class TD3_ACAgent():\n",
        "    def __init__(self, state_size, action_size, pi_lr, vf_lr, tau=0.007, policy_noise=0.1, noise_clip=0.2, policy_freq=2):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.pi_lr = pi_lr\n",
        "        self.vf_lr = vf_lr\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "        self.tau = tau\n",
        "        self.total_steps = 0\n",
        "        self.replay_buffer = ReplayBuffer(10000, state_size, action_size)\n",
        "\n",
        "        # Actor Network\n",
        "        self.actor = TD3_PolicyNetwork(state_size, action_size).to(device)\n",
        "        self.actor_target = TD3_PolicyNetwork(state_size, action_size).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.pi_lr )\n",
        "\n",
        "        # Critic Networks\n",
        "        self.critic1 = TD3_ValueNetwork(state_size,action_size).to(device)\n",
        "        self.critic2 = TD3_ValueNetwork(state_size,action_size).to(device)\n",
        "        self.critic1_target = TD3_ValueNetwork(state_size,action_size).to(device)\n",
        "        self.critic2_target = TD3_ValueNetwork(state_size,action_size).to(device)\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.vf_lr)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.vf_lr)\n",
        "\n",
        "    def evaluate(self, env, n_rollouts=1):\n",
        "        # print(\"In side the evaluation function\")\n",
        "        rewards = []\n",
        "        for _ in range(n_rollouts):\n",
        "            state, _ = env.reset()\n",
        "            done = False\n",
        "            truncated = False\n",
        "            rewards.append(0)\n",
        "            while not done and not truncated:\n",
        "                # state = self.normalize_state(state)\n",
        "                action = self.actor.select_action(state)\n",
        "                # print(action)\n",
        "                state, reward, done, truncated,_ = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
        "                rewards[-1] += reward\n",
        "        return np.mean(rewards), np.std(rewards)\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "            if len(self.replay_buffer) < batch_size:\n",
        "                return\n",
        "            # print(\"inside the learn method\")\n",
        "            # Sample a batch from the replay buffer\n",
        "            state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
        "\n",
        "            # Convert to tensors\n",
        "            # state = torch.FloatTensor(state).to(device)\n",
        "            action = torch.tensor(action).float().to(device)\n",
        "            reward = torch.tensor(reward).float().reshape(-1, 1).to(device)\n",
        "            # next_state = torch.FloatTensor(next_state).to(device)\n",
        "            # print(action.shape,reward.shape)\n",
        "            done = torch.tensor(done).float().reshape(-1, 1).to(device)\n",
        "\n",
        "            # Select next action according to target policy with noise\n",
        "            noise = torch.tensor(np.random.normal(0, self.policy_noise, size=(batch_size, self.action_size))).float().to(device)\n",
        "            # noise = noise.clamp(-self.noise_clip, self.noise_clip)\n",
        "            # print(\"next_state.shape\",next_state.shape)\n",
        "            temp_action = self.actor_target.evaluate_action(next_state)\n",
        "            # # print(temp_action.shape,\"temp_action.shape\")\n",
        "\n",
        "            temp_action = torch.from_numpy(temp_action).float().to(device)\n",
        "            # next_action = ( temp_action + noise).clamp(-1, 1)\n",
        "\n",
        "            temp_action = temp_action + noise\n",
        "\n",
        "                  # Apply the appropriate clamp to each action for the batch\n",
        "            first_action = temp_action[:, 0].clamp(-1, 1)  # First action (ranges from -1 to 1)\n",
        "            second_action = temp_action[:, 1].clamp(0, 1)  # Second action (ranges from 0 to 1)\n",
        "            third_action = temp_action[:, 2].clamp(0, 1)   # Third action (ranges from 0 to 1)\n",
        "\n",
        "            # Combine the actions back together into a tensor\n",
        "            next_action = torch.stack([first_action, second_action, third_action], dim=1)\n",
        "\n",
        "\n",
        "            # Compute target Q-value\n",
        "            target_Q1 = self.critic1_target(next_state, next_action)\n",
        "            target_Q2 = self.critic2_target(next_state, next_action)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + (1 - done) * self.gamma * target_Q\n",
        "\n",
        "            # Optimize Critic 1\n",
        "            current_Q1 = self.critic1(state, action)\n",
        "            loss_Q1 = F.mse_loss(current_Q1, target_Q.detach())\n",
        "            self.critic1_optimizer.zero_grad()\n",
        "            loss_Q1.backward()\n",
        "            self.critic1_optimizer.step()\n",
        "\n",
        "            # Optimize Critic 2\n",
        "            current_Q2 = self.critic2(state, action)\n",
        "            loss_Q2 = F.mse_loss(current_Q2, target_Q.detach())\n",
        "            self.critic2_optimizer.zero_grad()\n",
        "            loss_Q2.backward()\n",
        "            self.critic2_optimizer.step()\n",
        "\n",
        "            # Delayed policy updates\n",
        "            if self.total_steps % self.policy_freq == 0:\n",
        "                # Compute actor loss\n",
        "                # print(\"state.shape\",state.shape)\n",
        "                t_act = self.actor.evaluate_action(state)\n",
        "                t_act = torch.from_numpy(t_act).float().to(device)\n",
        "\n",
        "                fa = t_act[:, 0].clamp(-1, 1)  # First action (ranges from -1 to 1)\n",
        "                sa = t_act[:, 1].clamp(0, 1)  # Second action (ranges from 0 to 1)\n",
        "                ta = t_act[:, 2].clamp(0, 1)   # Third action (ranges from 0 to 1)\n",
        "                 # Combine the actions back together into a tensor\n",
        "                t_act_u = torch.stack([fa, sa, ta], dim=1)\n",
        "\n",
        "\n",
        "                actor_loss = -self.critic1(state,t_act_u).mean()\n",
        "\n",
        "                # Optimize the actor\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Soft update target networks\n",
        "                self.soft_update(self.actor, self.actor_target)\n",
        "                self.soft_update(self.critic1, self.critic1_target)\n",
        "                self.soft_update(self.critic2, self.critic2_target)\n",
        "\n",
        "    def soft_update(self, local_model, target_model):\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    # def normalize_state(self, state):\n",
        "    #     return state /255\n",
        "\n",
        "    def train(self, env1,env2, seed, gamma, max_episodes, max_episode_steps, batch_size, goal_mean_100_reward):\n",
        "        self.seed = seed\n",
        "        self.gamma = gamma\n",
        "        torch.manual_seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "        random.seed(self.seed)\n",
        "\n",
        "        self.episode_reward = []\n",
        "        self.evaluation_scores = []\n",
        "        self.total_steps = 0\n",
        "        result = np.empty((max_episodes, 3))\n",
        "        result[:] = np.nan\n",
        "        env1 = FrameStackWrapper(env1, num_frames=4)\n",
        "        # env2 = FrameStackWrapper(env2, num_frames=4)\n",
        "\n",
        "        for episode in range(1, max_episodes + 1):\n",
        "            state1, _ = env1.reset()\n",
        "            # state2, _ = env2.reset()\n",
        "\n",
        "            episode_reward = 0\n",
        "            episode_steps = 0\n",
        "\n",
        "            while episode_steps < max_episode_steps:\n",
        "                # if(episode_steps>max_episode_steps):\n",
        "                # print(\"I am falling into infinite\",episode_steps,max_episode_steps)\n",
        "                action1 = self.actor.select_action(state1)\n",
        "                # action2 = self.actor.select_action(state2)\n",
        "\n",
        "                action1 = action1 + np.random.normal(0, 0.15, size=self.action_size)\n",
        "                # action2 = action2 + np.random.normal(0, 0.15, size=self.action_size)\n",
        "\n",
        "                # print(action,state.shape)\n",
        "                next_state1, reward1, done1, truncated1, _ = env1.step(action1 * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
        "                # next_state2, reward2, done2, truncated2, _ = env1.step(action1 * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
        "\n",
        "                episode_reward += reward1\n",
        "                episode_steps += 1\n",
        "                self.total_steps += 1\n",
        "\n",
        "                self.replay_buffer.add(state1, action1, reward1, next_state1, done1)\n",
        "                # self.replay_buffer.add(state2, action2, reward2, next_state2, done2)\n",
        "\n",
        "                state1 = next_state1\n",
        "                # state2 = next_state2\n",
        "\n",
        "                if self.total_steps > batch_size:\n",
        "                    self.learn(batch_size)\n",
        "                max_out = episode_steps == max_episode_steps\n",
        "                if done1 or truncated1 or max_out:\n",
        "                    break\n",
        "\n",
        "            self.episode_reward.append(episode_reward)\n",
        "\n",
        "            # Bookkeeping\n",
        "            evaluation_score, _ = self.evaluate(env1)  # You need to implement this method\n",
        "            self.evaluation_scores.append(evaluation_score)\n",
        "\n",
        "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
        "            std_100_reward = np.std(self.episode_reward[-100:])\n",
        "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
        "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
        "\n",
        "            result[episode-1] = self.total_steps, mean_100_reward, mean_100_eval_score\n",
        "\n",
        "            print('Episode: {:d}, Total Steps: {:d}, Train_reward: {:.2f}, Eval_reward: {:.2f}'.format(\n",
        "                episode, self.total_steps, mean_100_reward, mean_100_eval_score))\n",
        "\n",
        "            # Check if the problem is considered solved\n",
        "            training_is_over = episode >= max_episodes or \\\n",
        "                               (goal_mean_100_reward is not None and mean_100_eval_score >= goal_mean_100_reward)\n",
        "            if training_is_over:\n",
        "                if episode >= max_episodes:\n",
        "                    print('--> reached_max_episodes')\n",
        "                if goal_mean_100_reward is not None and mean_100_eval_score >= goal_mean_100_reward:\n",
        "                    print('Environment solved in {:d} steps!\\tAverage Score: {:.2f}'.format(self.total_steps, mean_100_eval_score))\n",
        "                break\n",
        "\n",
        "        return np.array(result)\n"
      ],
      "metadata": {
        "id": "8XTmqCnLE1L4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "env1 = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.25, domain_randomize=False, continuous=True)\n",
        "env2 = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.25, domain_randomize=False, continuous=True)\n",
        "\n",
        "pi_lr = 7e-4\n",
        "vf_lr = 1e-3\n",
        "gamma = .99\n",
        "max_episodes = 200\n",
        "max_episode_steps = 150 #env1.spec.max_episode_steps  # you could set your own time limit\n",
        "goal_mean_100_reward = env1.spec.reward_threshold # that's Gym specific\n",
        "seed = 12\n",
        "batch_size = 256\n",
        "tau = 0.08 # how much new weights would be considered /// current run is with 0.007\n",
        "policy_noise = 0.1 # policy noise to randomize the actions\n",
        "noise_clip = 0.1 # clipping the actions\n",
        "policy_freq = 2\n",
        "# env1.seed(seed)\n",
        "td3agent = TD3_ACAgent(env1.observation_space.shape, env1.action_space.shape[0], pi_lr, vf_lr,tau, policy_noise, noise_clip, policy_freq)\n",
        "result_td3 = td3agent.train(env1,env2, seed, gamma, max_episodes, max_episode_steps,batch_size, goal_mean_100_reward)\n",
        "\n",
        "env1.close()\n",
        "del env1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6-bXlBiYK-S",
        "outputId": "88a6b51b-202a-4ce8-d65f-2cc2b1a70e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Steps: 28, Train_reward: -2.98, Eval_reward: -46.79\n",
            "Episode: 2, Total Steps: 178, Train_reward: -17.55, Eval_reward: -43.97\n",
            "Episode: 3, Total Steps: 218, Train_reward: -14.22, Eval_reward: -41.18\n",
            "Episode: 4, Total Steps: 244, Train_reward: -11.23, Eval_reward: -40.00\n",
            "Episode: 5, Total Steps: 394, Train_reward: -15.38, Eval_reward: -41.76\n",
            "Episode: 6, Total Steps: 544, Train_reward: -17.62, Eval_reward: -39.52\n",
            "Episode: 7, Total Steps: 653, Train_reward: -18.54, Eval_reward: -39.17\n",
            "Episode: 8, Total Steps: 714, Train_reward: -17.90, Eval_reward: -39.32\n",
            "Episode: 9, Total Steps: 753, Train_reward: -16.53, Eval_reward: -38.96\n",
            "Episode: 10, Total Steps: 835, Train_reward: -16.71, Eval_reward: -38.27\n",
            "Episode: 11, Total Steps: 963, Train_reward: -17.19, Eval_reward: -38.87\n",
            "Episode: 12, Total Steps: 1113, Train_reward: -19.31, Eval_reward: -39.27\n",
            "Episode: 13, Total Steps: 1196, Train_reward: -19.45, Eval_reward: -39.74\n",
            "Episode: 14, Total Steps: 1271, Train_reward: -19.25, Eval_reward: -39.79\n",
            "Episode: 15, Total Steps: 1352, Train_reward: -19.09, Eval_reward: -39.07\n",
            "Episode: 16, Total Steps: 1400, Train_reward: -18.48, Eval_reward: -39.37\n",
            "Episode: 17, Total Steps: 1426, Train_reward: -17.59, Eval_reward: -39.07\n",
            "Episode: 18, Total Steps: 1516, Train_reward: -17.49, Eval_reward: -38.49\n",
            "Episode: 19, Total Steps: 1642, Train_reward: -17.89, Eval_reward: -39.23\n",
            "Episode: 20, Total Steps: 1792, Train_reward: -18.25, Eval_reward: -39.20\n",
            "Episode: 21, Total Steps: 1820, Train_reward: -17.56, Eval_reward: -39.20\n",
            "Episode: 22, Total Steps: 1837, Train_reward: -16.77, Eval_reward: -38.78\n",
            "Episode: 23, Total Steps: 1904, Train_reward: -16.56, Eval_reward: -38.57\n",
            "Episode: 24, Total Steps: 1974, Train_reward: -16.44, Eval_reward: -38.09\n",
            "Episode: 25, Total Steps: 2063, Train_reward: -16.65, Eval_reward: -38.04\n",
            "Episode: 26, Total Steps: 2081, Train_reward: -16.03, Eval_reward: -38.12\n",
            "Episode: 27, Total Steps: 2231, Train_reward: -16.49, Eval_reward: -38.07\n",
            "Episode: 28, Total Steps: 2301, Train_reward: -16.49, Eval_reward: -37.75\n",
            "Episode: 29, Total Steps: 2319, Train_reward: -15.90, Eval_reward: -38.13\n",
            "Episode: 30, Total Steps: 2411, Train_reward: -16.04, Eval_reward: -38.37\n",
            "Episode: 31, Total Steps: 2466, Train_reward: -15.79, Eval_reward: -38.97\n",
            "Episode: 32, Total Steps: 2528, Train_reward: -15.66, Eval_reward: -38.86\n",
            "Episode: 33, Total Steps: 2678, Train_reward: -16.09, Eval_reward: -38.85\n",
            "Episode: 34, Total Steps: 2739, Train_reward: -15.96, Eval_reward: -38.78\n",
            "Episode: 35, Total Steps: 2845, Train_reward: -16.19, Eval_reward: -38.81\n",
            "Episode: 36, Total Steps: 2883, Train_reward: -15.86, Eval_reward: -38.76\n",
            "Episode: 37, Total Steps: 2900, Train_reward: -15.44, Eval_reward: -38.88\n",
            "Episode: 38, Total Steps: 3050, Train_reward: -15.95, Eval_reward: -38.94\n",
            "Episode: 39, Total Steps: 3110, Train_reward: -15.82, Eval_reward: -38.81\n",
            "Episode: 40, Total Steps: 3209, Train_reward: -15.96, Eval_reward: -38.83\n",
            "Episode: 41, Total Steps: 3227, Train_reward: -15.58, Eval_reward: -38.93\n",
            "Episode: 42, Total Steps: 3377, Train_reward: -16.18, Eval_reward: -38.80\n",
            "Episode: 43, Total Steps: 3394, Train_reward: -15.79, Eval_reward: -38.97\n",
            "Episode: 44, Total Steps: 3482, Train_reward: -15.81, Eval_reward: -39.06\n",
            "Episode: 45, Total Steps: 3599, Train_reward: -15.98, Eval_reward: -39.02\n",
            "Episode: 46, Total Steps: 3645, Train_reward: -15.76, Eval_reward: -39.08\n",
            "Episode: 47, Total Steps: 3712, Train_reward: -15.72, Eval_reward: -39.09\n",
            "Episode: 48, Total Steps: 3838, Train_reward: -16.11, Eval_reward: -38.91\n",
            "Episode: 49, Total Steps: 3951, Train_reward: -16.33, Eval_reward: -38.91\n",
            "Episode: 50, Total Steps: 4101, Train_reward: -16.64, Eval_reward: -38.72\n",
            "Episode: 51, Total Steps: 4142, Train_reward: -16.44, Eval_reward: -38.63\n",
            "Episode: 52, Total Steps: 4169, Train_reward: -16.11, Eval_reward: -38.79\n",
            "Episode: 53, Total Steps: 4281, Train_reward: -16.30, Eval_reward: -38.63\n",
            "Episode: 54, Total Steps: 4345, Train_reward: -16.26, Eval_reward: -38.58\n",
            "Episode: 55, Total Steps: 4495, Train_reward: -16.40, Eval_reward: -38.71\n",
            "Episode: 56, Total Steps: 4643, Train_reward: -16.69, Eval_reward: -38.70\n",
            "Episode: 57, Total Steps: 4681, Train_reward: -16.44, Eval_reward: -38.75\n",
            "Episode: 58, Total Steps: 4721, Train_reward: -16.32, Eval_reward: -38.53\n",
            "Episode: 59, Total Steps: 4753, Train_reward: -16.13, Eval_reward: -38.34\n",
            "Episode: 60, Total Steps: 4903, Train_reward: -16.49, Eval_reward: -38.17\n",
            "Episode: 61, Total Steps: 4986, Train_reward: -16.56, Eval_reward: -38.13\n",
            "Episode: 62, Total Steps: 5045, Train_reward: -16.44, Eval_reward: -38.12\n",
            "Episode: 63, Total Steps: 5092, Train_reward: -16.30, Eval_reward: -37.97\n",
            "Episode: 64, Total Steps: 5211, Train_reward: -16.43, Eval_reward: -37.93\n",
            "Episode: 65, Total Steps: 5353, Train_reward: -16.77, Eval_reward: -37.90\n",
            "Episode: 66, Total Steps: 5482, Train_reward: -16.97, Eval_reward: -37.95\n",
            "Episode: 67, Total Steps: 5563, Train_reward: -16.96, Eval_reward: -37.83\n",
            "Episode: 68, Total Steps: 5590, Train_reward: -16.74, Eval_reward: -37.88\n",
            "Episode: 69, Total Steps: 5706, Train_reward: -16.85, Eval_reward: -37.81\n",
            "Episode: 70, Total Steps: 5725, Train_reward: -16.62, Eval_reward: -37.92\n",
            "Episode: 71, Total Steps: 5807, Train_reward: -16.66, Eval_reward: -37.99\n",
            "Episode: 72, Total Steps: 5942, Train_reward: -16.86, Eval_reward: -37.93\n",
            "Episode: 73, Total Steps: 6013, Train_reward: -16.85, Eval_reward: -37.85\n",
            "Episode: 74, Total Steps: 6071, Train_reward: -16.78, Eval_reward: -37.78\n",
            "Episode: 75, Total Steps: 6115, Train_reward: -16.62, Eval_reward: -37.78\n",
            "Episode: 76, Total Steps: 6164, Train_reward: -16.51, Eval_reward: -38.03\n",
            "Episode: 77, Total Steps: 6240, Train_reward: -16.51, Eval_reward: -37.88\n",
            "Episode: 78, Total Steps: 6390, Train_reward: -16.79, Eval_reward: -37.89\n",
            "Episode: 79, Total Steps: 6540, Train_reward: -17.09, Eval_reward: -37.84\n",
            "Episode: 80, Total Steps: 6577, Train_reward: -16.95, Eval_reward: -37.88\n",
            "Episode: 81, Total Steps: 6603, Train_reward: -16.76, Eval_reward: -37.88\n",
            "Episode: 82, Total Steps: 6622, Train_reward: -16.57, Eval_reward: -37.66\n",
            "Episode: 83, Total Steps: 6736, Train_reward: -16.72, Eval_reward: -37.65\n",
            "Episode: 84, Total Steps: 6764, Train_reward: -16.54, Eval_reward: -37.60\n",
            "Episode: 85, Total Steps: 6833, Train_reward: -16.52, Eval_reward: -37.59\n",
            "Episode: 86, Total Steps: 6983, Train_reward: -16.75, Eval_reward: -37.53\n",
            "Episode: 87, Total Steps: 7100, Train_reward: -16.90, Eval_reward: -37.55\n",
            "Episode: 88, Total Steps: 7200, Train_reward: -16.90, Eval_reward: -37.54\n",
            "Episode: 89, Total Steps: 7266, Train_reward: -16.87, Eval_reward: -37.53\n",
            "Episode: 90, Total Steps: 7357, Train_reward: -16.91, Eval_reward: -37.29\n",
            "Episode: 91, Total Steps: 7374, Train_reward: -16.67, Eval_reward: -37.41\n",
            "Episode: 92, Total Steps: 7404, Train_reward: -16.55, Eval_reward: -37.54\n",
            "Episode: 93, Total Steps: 7423, Train_reward: -16.38, Eval_reward: -37.58\n",
            "Episode: 94, Total Steps: 7463, Train_reward: -16.28, Eval_reward: -37.62\n",
            "Episode: 95, Total Steps: 7480, Train_reward: -16.10, Eval_reward: -37.69\n",
            "Episode: 96, Total Steps: 7499, Train_reward: -15.93, Eval_reward: -37.82\n",
            "Episode: 97, Total Steps: 7548, Train_reward: -15.82, Eval_reward: -37.79\n",
            "Episode: 98, Total Steps: 7606, Train_reward: -15.76, Eval_reward: -37.77\n",
            "Episode: 99, Total Steps: 7632, Train_reward: -15.63, Eval_reward: -37.79\n",
            "Episode: 100, Total Steps: 7679, Train_reward: -15.55, Eval_reward: -37.86\n",
            "Episode: 101, Total Steps: 7697, Train_reward: -15.53, Eval_reward: -37.69\n",
            "Episode: 102, Total Steps: 7755, Train_reward: -15.32, Eval_reward: -37.61\n",
            "Episode: 103, Total Steps: 7807, Train_reward: -15.32, Eval_reward: -37.56\n",
            "Episode: 104, Total Steps: 7910, Train_reward: -15.54, Eval_reward: -37.67\n",
            "Episode: 105, Total Steps: 7938, Train_reward: -15.24, Eval_reward: -37.56\n",
            "Episode: 106, Total Steps: 8088, Train_reward: -15.26, Eval_reward: -37.62\n",
            "Episode: 107, Total Steps: 8210, Train_reward: -15.32, Eval_reward: -37.41\n",
            "Episode: 108, Total Steps: 8247, Train_reward: -15.22, Eval_reward: -37.38\n",
            "Episode: 109, Total Steps: 8397, Train_reward: -15.53, Eval_reward: -37.36\n",
            "Episode: 110, Total Steps: 8524, Train_reward: -15.66, Eval_reward: -37.31\n",
            "Episode: 111, Total Steps: 8674, Train_reward: -15.83, Eval_reward: -37.27\n",
            "Episode: 112, Total Steps: 8771, Train_reward: -15.68, Eval_reward: -37.24\n",
            "Episode: 113, Total Steps: 8886, Train_reward: -15.73, Eval_reward: -37.14\n",
            "Episode: 114, Total Steps: 8903, Train_reward: -15.56, Eval_reward: -37.11\n",
            "Episode: 115, Total Steps: 9053, Train_reward: -15.69, Eval_reward: -37.25\n",
            "Episode: 116, Total Steps: 9164, Train_reward: -15.85, Eval_reward: -37.20\n",
            "Episode: 117, Total Steps: 9270, Train_reward: -16.07, Eval_reward: -37.35\n",
            "Episode: 118, Total Steps: 9337, Train_reward: -16.04, Eval_reward: -37.53\n",
            "Episode: 119, Total Steps: 9415, Train_reward: -15.92, Eval_reward: -37.46\n",
            "Episode: 120, Total Steps: 9565, Train_reward: -15.97, Eval_reward: -37.49\n",
            "Episode: 121, Total Steps: 9584, Train_reward: -15.94, Eval_reward: -37.58\n",
            "Episode: 122, Total Steps: 9734, Train_reward: -16.23, Eval_reward: -37.61\n",
            "Episode: 124, Total Steps: 9904, Train_reward: -16.39, Eval_reward: -37.72\n",
            "Episode: 125, Total Steps: 10013, Train_reward: -16.42, Eval_reward: -37.62\n",
            "Episode: 126, Total Steps: 10146, Train_reward: -16.73, Eval_reward: -37.51\n",
            "Episode: 127, Total Steps: 10182, Train_reward: -16.49, Eval_reward: -37.41\n",
            "Episode: 128, Total Steps: 10212, Train_reward: -16.39, Eval_reward: -37.41\n",
            "Episode: 129, Total Steps: 10362, Train_reward: -16.76, Eval_reward: -37.26\n",
            "Episode: 130, Total Steps: 10388, Train_reward: -16.59, Eval_reward: -37.35\n",
            "Episode: 131, Total Steps: 10449, Train_reward: -16.63, Eval_reward: -37.18\n",
            "Episode: 132, Total Steps: 10486, Train_reward: -16.55, Eval_reward: -37.14\n",
            "Episode: 133, Total Steps: 10503, Train_reward: -16.25, Eval_reward: -37.13\n",
            "Episode: 134, Total Steps: 10567, Train_reward: -16.28, Eval_reward: -37.20\n",
            "Episode: 135, Total Steps: 10584, Train_reward: -16.03, Eval_reward: -37.10\n",
            "Episode: 136, Total Steps: 10612, Train_reward: -16.02, Eval_reward: -37.16\n",
            "Episode: 137, Total Steps: 10694, Train_reward: -16.19, Eval_reward: -37.02\n",
            "Episode: 138, Total Steps: 10711, Train_reward: -15.83, Eval_reward: -36.90\n",
            "Episode: 139, Total Steps: 10819, Train_reward: -16.02, Eval_reward: -36.88\n",
            "Episode: 140, Total Steps: 10836, Train_reward: -15.80, Eval_reward: -36.84\n",
            "Episode: 141, Total Steps: 10936, Train_reward: -16.06, Eval_reward: -36.71\n",
            "Episode: 142, Total Steps: 11086, Train_reward: -16.00, Eval_reward: -36.77\n",
            "Episode: 143, Total Steps: 11224, Train_reward: -16.42, Eval_reward: -36.74\n",
            "Episode: 144, Total Steps: 11366, Train_reward: -16.57, Eval_reward: -36.69\n",
            "Episode: 145, Total Steps: 11516, Train_reward: -16.64, Eval_reward: -36.77\n",
            "Episode: 146, Total Steps: 11555, Train_reward: -16.64, Eval_reward: -36.74\n",
            "Episode: 147, Total Steps: 11615, Train_reward: -16.61, Eval_reward: -36.68\n",
            "Episode: 148, Total Steps: 11761, Train_reward: -16.66, Eval_reward: -36.67\n",
            "Episode: 149, Total Steps: 11859, Train_reward: -16.57, Eval_reward: -36.62\n",
            "Episode: 150, Total Steps: 12009, Train_reward: -16.61, Eval_reward: -36.67\n",
            "Episode: 151, Total Steps: 12131, Train_reward: -16.82, Eval_reward: -36.80\n",
            "Episode: 152, Total Steps: 12214, Train_reward: -17.04, Eval_reward: -36.63\n",
            "Episode: 153, Total Steps: 12250, Train_reward: -16.81, Eval_reward: -36.67\n",
            "Episode: 154, Total Steps: 12400, Train_reward: -16.99, Eval_reward: -36.71\n",
            "Episode: 155, Total Steps: 12550, Train_reward: -17.05, Eval_reward: -36.67\n",
            "Episode: 156, Total Steps: 12700, Train_reward: -16.96, Eval_reward: -36.80\n",
            "Episode: 157, Total Steps: 12850, Train_reward: -17.31, Eval_reward: -36.81\n",
            "Episode: 158, Total Steps: 12889, Train_reward: -17.27, Eval_reward: -36.89\n",
            "Episode: 159, Total Steps: 12906, Train_reward: -17.22, Eval_reward: -36.92\n",
            "Episode: 160, Total Steps: 12923, Train_reward: -16.84, Eval_reward: -37.02\n",
            "Episode: 161, Total Steps: 12968, Train_reward: -16.70, Eval_reward: -37.12\n",
            "Episode: 162, Total Steps: 12997, Train_reward: -16.63, Eval_reward: -37.03\n",
            "Episode: 163, Total Steps: 13017, Train_reward: -16.57, Eval_reward: -37.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env1 = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.75, domain_randomize=False, continuous=True)\n",
        "\n",
        "env2 = FrameStackWrapper(env1, num_frames=4)\n",
        "\n",
        "images = []\n",
        "s,_ = env2.reset()\n",
        "print(s.shape)\n",
        "img = env2.render()\n",
        "# print(img.shape)\n",
        "# plt.imshow(img)  # if state is a 3D tensor (e.g., (1, H, W, C))\n",
        "# plt.show()\n",
        "images.append(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMEzvvlIrH6T",
        "outputId": "0fd2bb51-478c-44ef-f1a8-7abc3d2b72f2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 96, 96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_results(array, filename, mode=0):\n",
        "    \"\"\"\n",
        "    Save a model to a file in your colab space\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: your Q network\n",
        "    filename: the name of the checkpoint file\n",
        "    mode (int): determines where to store the file\n",
        "                --> 0: collab space 1: Google Drive\n",
        "    \"\"\"\n",
        "    if mode == 1:\n",
        "      drive.mount('/content/gdrive')\n",
        "      path = F\"/content/gdrive/My Drive/Colab_workspace/{filename}\"\n",
        "      np.save(path, array)\n",
        "\n",
        "    else:\n",
        "      np.save(filename, array)"
      ],
      "metadata": {
        "id": "g4YdbWwxU3A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_results(result_ppo,'result_PPO_race_car2.npy')"
      ],
      "metadata": {
        "id": "8CXlFD7IpTnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_checkpoint(ppoagent.pnetwork,'ppo_pnetwork_race_car2.pt')\n",
        "save_checkpoint(ppoagent.vnetwork,'ppo_vnetwork_race_car2.pt')"
      ],
      "metadata": {
        "id": "9beGRowZVD70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_to_local_drive('result_PPO_race_car2.npy')\n",
        "export_to_local_drive('ppo_pnetwork_race_car2.pt')\n",
        "export_to_local_drive('ppo_vnetwork_race_car2.pt')"
      ],
      "metadata": {
        "id": "vev8FHbzVHGS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}